{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5LQDL_FLwnUI",
   "metadata": {
    "id": "5LQDL_FLwnUI"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8479c177",
   "metadata": {
    "id": "8479c177"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "af09c070",
   "metadata": {
    "id": "af09c070"
   },
   "outputs": [],
   "source": [
    "# Read the data and build vocabulary\n",
    "class DatasetLoader:\n",
    "    def __init__(self, filename):\n",
    "        self.raw_data = self.read_data(filename)\n",
    "        self.vocab = {}\n",
    "        self.tokenized_data = []\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        tokenized_data = []\n",
    "        for sentence in data:\n",
    "            tokenized_data.append(['<START>'] + sentence.split() + ['<STOP>'])\n",
    "        return tokenized_data\n",
    "        \n",
    "    def read_data(self, filename):\n",
    "        with open(cwd + '/data/' + filename, \"r\", encoding=\"UTF-8\") as f:\n",
    "            lines = [line.rstrip() for line in f]\n",
    "        return lines\n",
    "    \n",
    "    def process(self):\n",
    "        # tokenize data\n",
    "        self.data = self.tokenize(self.raw_data)\n",
    "\n",
    "        # create a dictionary of tokens with their counts\n",
    "        for sentence in self.data:\n",
    "            for word in sentence:\n",
    "                self.vocab[word] = self.vocab.get(word, 0) + 1\n",
    "\n",
    "        # remove tokens with count less than 3\n",
    "        for t in self.vocab:\n",
    "            if self.vocab[t] < 3:\n",
    "                self.vocab[t] = self.vocab.get(t, 0) + 1\n",
    "                self.vocab[t] = 0\n",
    "        self.vocab = {k: v for (k, v) in self.vocab.items() if v > 0}\n",
    "\n",
    "        # replace tokens with count less than 3 with <UNK>\n",
    "        for sentence in self.data:\n",
    "            for i, word in enumerate(sentence):\n",
    "                if self.vocab.get(word, 0) < 3:\n",
    "                    sentence[i] = '<UNK>'\n",
    "                    self.vocab['<UNK>'] = self.vocab.get('<UNK>', 0) + 1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1fed20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  26602\n"
     ]
    }
   ],
   "source": [
    "# Load the training data, tokenize it, and create a vocabulary\n",
    "dataset = DatasetLoader(\"1b_benchmark.train.tokens\")\n",
    "dataset.process()\n",
    "\n",
    "print(\"Vocabulary size: \", len(dataset.vocab)-1)  # -1 for <START> token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5ca2e",
   "metadata": {},
   "source": [
    "##### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d55d0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language model\n",
    "class LanguageModel:\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.M = 0  # total tokens\n",
    "        self.vocab_size = len(vocab)\n",
    "        # unigrams, bigrams, and trigrams\n",
    "        self.unigrams = {}\n",
    "        self.bigrams = {}\n",
    "        self.trigrams = {}\n",
    "        # probabilities for unigrams, bigrams, and trigrams\n",
    "        self.unigram_probs = {}\n",
    "        self.bigram_probs = {}\n",
    "        self.trigram_probs = {}\n",
    "        \n",
    "\n",
    "# we need the language model to be a class-based to get object from it\n",
    "# and use it for train, test, and dev data\n",
    "\n",
    "# create a language model\n",
    "train_lm = LanguageModel(dataset.data, dataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba5a8b",
   "metadata": {},
   "source": [
    "##### Create N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6a233ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the training data:  1622905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61530/61530 [00:03<00:00, 19559.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unigrams:  26603\n",
      "# of bigrams:  510391\n",
      "# of trigrams:  1111319\n"
     ]
    }
   ],
   "source": [
    "  # find total number of tokens \n",
    "    # present in the train data\n",
    "def find_total_tokens(lm):\n",
    "    counter = 0\n",
    "    for sentence in lm.data:\n",
    "        counter += len(sentence)-1\n",
    "    return counter\n",
    "\n",
    "train_lm.M = find_total_tokens(train_lm)\n",
    "print(\"Total number of tokens in the training data: \", train_lm.M)\n",
    "\n",
    "\n",
    "def create_ngrams(sentences):\n",
    "    unigrams, bigrams, trigrams = {}, {}, {}\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        for i in range(len(sentence)):\n",
    "            # unigrams\n",
    "            unigrams[sentence[i]] = unigrams.get(sentence[i], 0) + 1\n",
    "\n",
    "            # bigrams\n",
    "            if i < len(sentence) - 1:\n",
    "                bigram = (sentence[i], sentence[i+1])\n",
    "                bigrams[bigram] = bigrams.get(bigram, 0) + 1\n",
    "\n",
    "            # trigrams\n",
    "            if i < len(sentence) - 2:\n",
    "                trigram = (sentence[i], sentence[i+1], sentence[i+2])\n",
    "                trigrams[trigram] = trigrams.get(trigram, 0) + 1\n",
    "\n",
    "    return unigrams, bigrams, trigrams\n",
    "\n",
    "\n",
    "# create unigrams, bigrams, and trigrams\n",
    "unigrams, bigrams, trigrams = create_ngrams(train_lm.data)\n",
    "train_lm.unigrams = unigrams\n",
    "train_lm.bigrams = bigrams\n",
    "train_lm.trigrams = trigrams\n",
    "\n",
    "\n",
    "print(\"# of unigrams: \", len(unigrams))\n",
    "print(\"# of bigrams: \", len(bigrams))\n",
    "print(\"# of trigrams: \", len(trigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fe559",
   "metadata": {},
   "source": [
    "##### Getting probabilities of unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ae3efc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc-ing unigram probs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26603/26603 [00:00<00:00, 1474126.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc-ing bigram probs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510391/510391 [00:00<00:00, 590503.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc-ing trigram probs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1111319/1111319 [00:02<00:00, 371077.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get unigram probabilities\n",
    "def get_unigram_probs(lm):\n",
    "    probs = {}\n",
    "    for u in tqdm(lm.unigrams):\n",
    "        probs[u] = unigrams[u] / lm.M\n",
    "    return probs\n",
    "\n",
    "\n",
    "# get bigram probabilities\n",
    "def find_probs_bigram(lm):\n",
    "    probs = {}\n",
    "    # find the probability for each unique bigram\n",
    "    for b in tqdm(lm.bigrams):\n",
    "        if b[0] not in lm.unigrams:\n",
    "            probs[b] = 0\n",
    "        else:\n",
    "            probs[b] = lm.bigrams[b] / lm.unigrams[b[0]]\n",
    "    return probs\n",
    "\n",
    "\n",
    "# get trigram probabilities\n",
    "def find_probs_trigram(lm):\n",
    "    probs = {}\n",
    "    # find the probability for each unique trigram\n",
    "    for t in tqdm(lm.trigrams):\n",
    "        if t[0:2] not in lm.bigrams:\n",
    "            probs[t] = 0\n",
    "        else:\n",
    "            probs[t] = lm.trigrams[t] / lm.bigrams[t[0:2]]\n",
    "    return probs\n",
    "\n",
    "\n",
    "# get unigram, bigram, and trigram probabilities\n",
    "print(\"Calc-ing unigram probs ...\")\n",
    "train_lm.unigram_probs = get_unigram_probs(train_lm)\n",
    "print(\"Calc-ing bigram probs ...\")\n",
    "train_lm.bigram_probs = find_probs_bigram(train_lm)\n",
    "print(\"Calc-ing trigram probs ...\")\n",
    "train_lm.trigram_probs = find_probs_trigram(train_lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1349ea8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram probabilities: \n",
      " [(('last', 'year'), 0.23150816522574447), (('year', '.'), 0.23281393217231897), (('<START>', 'Bush'), 0.000520071509832602), (('Bush', 'is'), 0.04722222222222222), (('is', 'remembered'), 0.0002680246582685607)]\n",
      "\n",
      "Trigram probabilities: \n",
      " [((\"'\", 'Coming', 'Home'), 1.0), (('Coming', 'Home', \"'\"), 0.5), (('Home', \"'\", 'was'), 1.0), ((\"'\", 'was', 'released'), 0.4), (('was', 'released', 'in'), 0.11428571428571428)]\n"
     ]
    }
   ],
   "source": [
    "st = 400\n",
    "\n",
    "p = list(train_lm.bigram_probs.items())[st:st+5]\n",
    "print(\"Bigram probabilities: \\n\", p)\n",
    "\n",
    "print()\n",
    "\n",
    "t = list(train_lm.trigram_probs.items())[st:st+5]\n",
    "print(\"Trigram probabilities: \\n\", t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc4ea7",
   "metadata": {},
   "source": [
    "##### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be9e73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of unigram model:  976.5437422251438\n",
      "Perplexity of bigram model:  77.07346595596329\n",
      "Perplexity of trigram model:  6.55994749293112\n"
     ]
    }
   ],
   "source": [
    "# Now we can calculate the perplexity\n",
    "\n",
    "def get_log_prob(prob):\n",
    "    if prob == 0:  # if prob == 0, log(prob) = -inf\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log2(prob)\n",
    "\n",
    "\n",
    "def get_M(data):\n",
    "    M = 0\n",
    "    for sentence in data:\n",
    "        M += len(sentence) - 1\n",
    "    return M\n",
    "\n",
    "\n",
    "# calculate perplexity of unigram model\n",
    "def unigram_perplexity(lm, data):\n",
    "    log_prob = 0\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word == '<START>':\n",
    "                continue\n",
    "            log_prob += get_log_prob(lm.unigram_probs.get(word, 0))\n",
    "    return math.pow(2, -log_prob / get_M(data))\n",
    "\n",
    "\n",
    "# calculate perplexity of bigram model\n",
    "def bigram_perplexity(lm, data):\n",
    "    log_prob = 0\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):            \n",
    "            bigram = (sentence[i-1], sentence[i])\n",
    "            log_prob += get_log_prob(lm.bigram_probs.get(bigram, 0))\n",
    "    return math.pow(2, -log_prob / get_M(data))\n",
    "\n",
    "\n",
    "\n",
    "# calculate perplexity of trigram model\n",
    "def trigram_perplexity(lm, data):\n",
    "    log_prob = 0\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            # if the word is <START>, use the bigram of 2nd and 3rd word\n",
    "            if \"<START>\" in sentence[i-2:i]:\n",
    "                b = (sentence[i-1], sentence[i])\n",
    "                prob = lm.bigram_probs.get(b, 0)\n",
    "            else:\n",
    "                t = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "                prob = lm.trigram_probs.get(t, 0)\n",
    "            log_prob += get_log_prob(prob)\n",
    "\n",
    "    return math.pow(2, -log_prob / get_M(data))\n",
    "\n",
    "\n",
    "# calculate perplexity of unigram, bigram, and trigram models\n",
    "print(\"Perplexity of unigram model: \", unigram_perplexity(train_lm, train_lm.data))\n",
    "print(\"Perplexity of bigram model: \", bigram_perplexity(train_lm, train_lm.data))\n",
    "print(\"Perplexity of trigram model: \", trigram_perplexity(train_lm, train_lm.data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5f9ef",
   "metadata": {},
   "source": [
    "##### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6dedca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear interpolation perplexity: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20.80836378514529"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def linear_interpolation_perplexity(lm, l1, l2, l3, data):\n",
    "    log_prob = 0\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            # trigram\n",
    "            # if the word is <START>, use the bigram of 2nd and 3rd word\n",
    "            if \"<START>\" in sentence[i-2:i]:\n",
    "                b = (sentence[i-1], sentence[i])\n",
    "                prob = l3 * lm.bigram_probs.get(b, 0)\n",
    "            else:\n",
    "                t = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "                prob = l3 * lm.trigram_probs.get(t, 0)\n",
    "            \n",
    "            # bigram\n",
    "            b = (sentence[i-1], sentence[i])\n",
    "            prob += l2 * lm.bigram_probs.get(b, 0)\n",
    "            \n",
    "            # unigram\n",
    "            if sentence[i] == \"<START>\":\n",
    "                prob += l1 * 0\n",
    "            else:\n",
    "                prob += l1 * lm.unigram_probs.get(sentence[i], 0)\n",
    "\n",
    "            log_prob += get_log_prob(prob)    \n",
    "\n",
    "    return math.pow(2, -log_prob / get_M(data))\n",
    "\n",
    "\n",
    "# calculate perplexity of unigram, bigram, and trigram models\n",
    "l_list = [\n",
    "    (0.5, 0.2, 0.3),\n",
    "    (0.1, 0.2, 0.7),\n",
    "    (0.1, 0.3, 0.6),\n",
    "    (0.2, 0.3, 0.5),\n",
    "    (0.3, 0.4, 0.3),\n",
    "]\n",
    "\n",
    "print(\"Linear interpolation perplexity: \")\n",
    "linear_interpolation_perplexity(train_lm, 0.5, 0.2, 0.3, train_lm.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe746f",
   "metadata": {},
   "source": [
    "##### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3ca18094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<START>', 'HDTV', '.', '<STOP>']]\n",
      "Debug mode: \n",
      "Perplexity of unigram model:  658.0445066285465\n",
      "Perplexity of bigram model:  63.70757362051903\n",
      "Perplexity of trigram model:  1.5874010519681994\n",
      "Linear interpolation perplexity:  98.63583306527288\n"
     ]
    }
   ],
   "source": [
    "# debug code\n",
    "\n",
    "def preprocess(sentences):\n",
    "    # preprocess the sentences\n",
    "    preprocessed = []\n",
    "    for s in sentences:\n",
    "        words = ['<START>'] + s.split() + ['<STOP>']\n",
    "        for word in words:\n",
    "            if word not in train_lm.vocab.keys():\n",
    "                words[words.index(word)] = '<UNK>'\n",
    "        preprocessed.append(words)\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "debug_data = ['HDTV .']\n",
    "p = preprocess(debug_data)\n",
    "print(p)\n",
    "print(\"Debug mode: \")\n",
    "print(\"Perplexity of unigram model: \", unigram_perplexity(train_lm, p))\n",
    "print(\"Perplexity of bigram model: \", bigram_perplexity(train_lm, p))\n",
    "print(\"Perplexity of trigram model: \", trigram_perplexity(train_lm, p))\n",
    "print(\"Linear interpolation perplexity: \", linear_interpolation_perplexity(train_lm, 0.1, 0.3, 0.6, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f50476",
   "metadata": {},
   "source": [
    "##### Dev and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4d4eed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataset perplexity: \n",
      "\n",
      "Perplexity of unigram model:  625.7605102630304\n",
      "Perplexity of bigram model:  32.77505030825911\n",
      "Perplexity of trigram model:  2.977545749514274\n",
      "Linear interpolation perplexity:  251.4977414572197\n",
      "\n",
      "\n",
      "Test dataset perplexity: \n",
      "\n",
      "Perplexity of unigram model:  625.7229264239381\n",
      "Perplexity of bigram model:  32.69482904011273\n",
      "Perplexity of trigram model:  2.9797598678973816\n",
      "Linear interpolation perplexity:  250.24339103648967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dev dataset\n",
    "dev_ds = DatasetLoader(\"1b_benchmark.dev.tokens\")\n",
    "dev_ds.process()\n",
    "\n",
    "dev_lm = LanguageModel(dev_ds.data, train_lm.vocab)\n",
    "\n",
    "print(\"Dev dataset perplexity: \\n\")\n",
    "print(\"Perplexity of unigram model: \", unigram_perplexity(train_lm, dev_lm.data))\n",
    "print(\"Perplexity of bigram model: \", bigram_perplexity(train_lm, dev_lm.data))\n",
    "print(\"Perplexity of trigram model: \", trigram_perplexity(train_lm, dev_lm.data))\n",
    "print(\"Linear interpolation perplexity: \", linear_interpolation_perplexity(train_lm, 0.1, 0.3, 0.6, dev_lm.data))\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# test dataset\n",
    "test_ds = DatasetLoader(\"1b_benchmark.test.tokens\")\n",
    "test_ds.process()\n",
    "\n",
    "test_lm = LanguageModel(test_ds.data, train_lm.vocab)\n",
    "\n",
    "print(\"Test dataset perplexity: \\n\")\n",
    "print(\"Perplexity of unigram model: \", unigram_perplexity(train_lm, test_lm.data))\n",
    "print(\"Perplexity of bigram model: \", bigram_perplexity(train_lm, test_lm.data))\n",
    "print(\"Perplexity of trigram model: \", trigram_perplexity(train_lm, test_lm.data))\n",
    "print(\"Linear interpolation perplexity: \", linear_interpolation_perplexity(train_lm, 0.1, 0.3, 0.6, test_lm.data))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "df5beed04677c87a00017d3c2fd6e941bb6c7ec5b0b0ae1bda778ec597676f60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
